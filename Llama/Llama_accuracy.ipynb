{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "models = {\n",
        "    'Llama_1': {\n",
        "        'y_true': [0,0,0,1,1,1,0,0,1,0,1,1,0,0,0,1],\n",
        "        'y_pred': [1,0,0,0,1,1,0,1,0,1,1,1,0,1,1,1]\n",
        "    },\n",
        "    'Llama_2': {\n",
        "        'y_true': [1,1,0,1,0,0,1,1,1,1,1],\n",
        "        'y_pred': [0,1,1,0,0,1,1,1,1,1,0]\n",
        "    },\n",
        "    'Llama_3': {\n",
        "        'y_true': [0,1,1,1,1,1,1,1,0,1,0,1,1,1],\n",
        "        'y_pred': [1,0,1,0,1,1,1,1,0,1,0,0,1,0]\n",
        "    },\n",
        "    'Llama_4': {\n",
        "        'y_true': [1,1,1,1,1,1,1,1,1,1,1,1],\n",
        "        'y_pred': [1,1,1,1,0,0,1,0,0,1,1,0]\n",
        "    },\n",
        "    'Llama_5': {\n",
        "        'y_true': [1,0,1,1,1,1,1,0,1,1,1],\n",
        "        'y_pred': [1,1,0,0,1,1,1,1,1,1,1]\n",
        "    },\n",
        "    'Llama_7': {\n",
        "        'y_true': [0,1,0,1,0,1,1,1,1,0,1],\n",
        "        'y_pred': [1,1,1,0,1,1,1,1,1,1,1]\n",
        "    },\n",
        "    'GPT_3.5': {\n",
        "        'y_true': [1,1,0,1,0,0,1,1,1],\n",
        "        'y_pred': [0,1,0,1,0,0,1,1,1]\n",
        "    }\n",
        "}\n",
        "\n",
        "model_metrics = {}\n",
        "\n",
        "for model, data in models.items():\n",
        "    y_true = data['y_true']\n",
        "    y_pred = data['y_pred']\n",
        "\n",
        "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "    f1 = metrics.f1_score(y_true, y_pred)\n",
        "    report = metrics.classification_report(y_true, y_pred)\n",
        "\n",
        "    model_metrics[model] = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "# Вывод метрик для каждой модели\n",
        "for model, data in model_metrics.items():\n",
        "    print(f'model: {model}')\n",
        "    print(f'accuracy: {{0:.4f}}'.format(data[\"accuracy\"]))\n",
        "    print(f'f1: {{0:.4f}}'.format(data[\"f1\"]))\n",
        "    print('report:\\n{}'.format(data[\"report\"]))\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLKdziV8EaKx",
        "outputId": "28ca3b13-5cfe-4145-d7a7-58ab1018fff8"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Llama_1\n",
            "accuracy: 0.5625\n",
            "f1: 0.5882\n",
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.44      0.53         9\n",
            "           1       0.50      0.71      0.59         7\n",
            "\n",
            "    accuracy                           0.56        16\n",
            "   macro avg       0.58      0.58      0.56        16\n",
            "weighted avg       0.59      0.56      0.56        16\n",
            "\n",
            "\n",
            "model: Llama_2\n",
            "accuracy: 0.5455\n",
            "f1: 0.6667\n",
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.33      0.29         3\n",
            "           1       0.71      0.62      0.67         8\n",
            "\n",
            "    accuracy                           0.55        11\n",
            "   macro avg       0.48      0.48      0.48        11\n",
            "weighted avg       0.59      0.55      0.56        11\n",
            "\n",
            "\n",
            "model: Llama_3\n",
            "accuracy: 0.6429\n",
            "f1: 0.7368\n",
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.67      0.44         3\n",
            "           1       0.88      0.64      0.74        11\n",
            "\n",
            "    accuracy                           0.64        14\n",
            "   macro avg       0.60      0.65      0.59        14\n",
            "weighted avg       0.76      0.64      0.67        14\n",
            "\n",
            "\n",
            "model: Llama_4\n",
            "accuracy: 0.5833\n",
            "f1: 0.7368\n",
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       1.00      0.58      0.74        12\n",
            "\n",
            "    accuracy                           0.58        12\n",
            "   macro avg       0.50      0.29      0.37        12\n",
            "weighted avg       1.00      0.58      0.74        12\n",
            "\n",
            "\n",
            "model: Llama_5\n",
            "accuracy: 0.6364\n",
            "f1: 0.7778\n",
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.78      0.78      0.78         9\n",
            "\n",
            "    accuracy                           0.64        11\n",
            "   macro avg       0.39      0.39      0.39        11\n",
            "weighted avg       0.64      0.64      0.64        11\n",
            "\n",
            "\n",
            "model: Llama_7\n",
            "accuracy: 0.5455\n",
            "f1: 0.7059\n",
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.60      0.86      0.71         7\n",
            "\n",
            "    accuracy                           0.55        11\n",
            "   macro avg       0.30      0.43      0.35        11\n",
            "weighted avg       0.38      0.55      0.45        11\n",
            "\n",
            "\n",
            "model: GPT_3.5\n",
            "accuracy: 0.8889\n",
            "f1: 0.9091\n",
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      1.00      0.86         3\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "    accuracy                           0.89         9\n",
            "   macro avg       0.88      0.92      0.88         9\n",
            "weighted avg       0.92      0.89      0.89         9\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT\n",
        "accuracy: 0.7700\n",
        "\n",
        "#print(metrics.f1_score(y_true, y_pred,average='micro'))\n",
        "0.7088607594936709\n",
        "\n",
        "AUROC per tag\n",
        "Correct review : 0.6\n",
        "Incorrect review : 0.43555555555555553\n",
        "\n",
        "\n",
        "                   precision    recall  f1-score   support\n",
        "\n",
        "  Correct review        0.97      0.62      0.76        45\n",
        "Incorrect review        0.00      0.00      0.00         5\n",
        "\n",
        "        micro avg       0.97      0.56      0.71        50\n",
        "        macro avg       0.48      0.31      0.38        50\n",
        "     weighted avg       0.87      0.56      0.68        50\n",
        "      samples avg       0.56      0.56      0.56        50\n",
        "\n",
        "AUROC per tag\n",
        "Correct review : 0.6\n",
        "Incorrect review : 0.43555555555555553"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "jt5Gxyx-ZDub",
        "outputId": "c345bd8e-b8d3-4dc8-8c65-f352d82f19d6"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    Correct review        0.97      0.62      0.76        45\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iA3DhsEtZGy8"
      },
      "execution_count": 87,
      "outputs": []
    }
  ]
}